{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51da5453",
   "metadata": {},
   "source": [
    "# General Linear Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad9424",
   "metadata": {},
   "source": [
    "# 1. What is the purpose of the General Linear Model (GLM)?\n",
    "\n",
    "The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e5276",
   "metadata": {},
   "source": [
    "# 2. What are the key assumptions of the General Linear Model?\n",
    "\n",
    "a)Linearity: The relationship between the dependent and independent variables is linear.\n",
    "\n",
    "b)Independence: The observations are independent of each other.\n",
    "\n",
    "c)Homoscedasticity: The variance of the errors is constant across all levels of the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b70be4",
   "metadata": {},
   "source": [
    "# 3. How do you interpret the coefficients in a GLM?\n",
    "\n",
    "Step 1: Determine whether the association between the response and the term is statistically significant.\n",
    "\n",
    "Step 2: Determine how well the model fits your data.\n",
    "\n",
    "Step 3: Determine whether your model meets the assumptions of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed197ec3",
   "metadata": {},
   "source": [
    "# 4. What is the difference between a univariate and multivariate GLM?\n",
    " \n",
    "Univariate analysis is the analysis of one variable. Multivariate analysis is the analysis of more than one variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb4594",
   "metadata": {},
   "source": [
    "# 5. Explain the concept of interaction effects in a GLM\n",
    "\n",
    "An interaction effect occurs when the effect of one variable depends on the value of another variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85ff05",
   "metadata": {},
   "source": [
    "# 6. How do you handle categorical predictors in a GLM?\n",
    "\n",
    "1)Drop Categorical Variables The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.\n",
    "\n",
    "2)Label Encoding Label encoding assigns each unique value to a different integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52467eb",
   "metadata": {},
   "source": [
    "# 7. What is the purpose of the design matrix in a GLM?\n",
    "\n",
    "The purpose of the design matrix is to allow models that further constrain parameter sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ceaf4",
   "metadata": {},
   "source": [
    "# 8. How do you test the significance of predictors in a GLM?\n",
    "\n",
    "A low p-value (< 0.05) indicates that you can reject the null hypothesis. In other words, a predictor that has a low p-value is likely to be a meaningful addition to your model because changes in the predictor's value are related to changes in the response variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d1eb9",
   "metadata": {},
   "source": [
    "# 9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?\n",
    "\n",
    "use Type I only when there is a serious theoretical reason for it, use Type II when there is no interaction, use Type III when there is interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aff679a",
   "metadata": {},
   "source": [
    "# 10. Explain the concept of deviance in a GLM\n",
    "\n",
    "Deviance. We see the word Deviance twice over in the model output. Deviance is a measure of goodness of fit of a generalized linear model. Or rather, it's a measure of badness of fit–higher numbers indicate worse fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378326e",
   "metadata": {},
   "source": [
    "# Regression:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583008b",
   "metadata": {},
   "source": [
    "# 11. What is regression analysis and what is its purpose?\n",
    "\n",
    "Regression analysis is a statistical method that shows the relationship between two or more variables. Usually expressed in a graph, the method tests the relationship between a dependent variable against independent variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61091f3b",
   "metadata": {},
   "source": [
    "# 12. What is the difference between simple linear regression and multiple linear regression?\n",
    "\n",
    "Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables. For instance, when we predict rent based on square feet alone that is simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea4c9f9",
   "metadata": {},
   "source": [
    "# 13. How do you interpret the R-squared value in regression?\n",
    "\n",
    "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively. R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 – 100% scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489652b8",
   "metadata": {},
   "source": [
    "# 14. What is the difference between correlation and regression?\n",
    "\n",
    "Correlation is a statistical measure that determines the association or co-relationship between two variables. Regression describes how to numerically relate an independent variable to the dependent variable. To represent a linear relationship between two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58553e44",
   "metadata": {},
   "source": [
    "# 15. What is the difference between the coefficients and the intercept in regression?\n",
    "\n",
    "The simple linear regression model is essentially a linear equation of the form y = c + b*x; where y is the dependent variable (outcome), x is the independent variable (predictor), b is the slope of the line; also known as regression coefficient and c is the intercept; labeled as constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8470e0bb",
   "metadata": {},
   "source": [
    "# 16. How do you handle outliers in regression analysis?\n",
    "\n",
    "There are many possible approaches to dealing with outliers: removing them from the observations, treating them (for example, capping the extreme observations at a reasonable value), or using algorithms that are well-suited for dealing with such values on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e686fe7",
   "metadata": {},
   "source": [
    "# 17. What is the difference between ridge regression and ordinary least squares regression?\n",
    "\n",
    "Ridge regression is a term used to refer to a linear regression model whose coefficients are estimated not by ordinary least squares (OLS), but by an estimator, called ridge estimator, that, albeit biased, has lower variance than the OLS estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e09f1",
   "metadata": {},
   "source": [
    "# 18. What is heteroscedasticity in regression and how does it affect the model?\n",
    "\n",
    "Heteroskedasticity refers to situations where the variance of the residuals is unequal over a range of measured values\n",
    "\n",
    "Heteroscedasticity makes a regression model less dependable because the residuals should not follow any specific pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec831f",
   "metadata": {},
   "source": [
    "# 19. How do you handle multicollinearity in regression analysis?\n",
    "\n",
    "1)Remove some of the highly correlated independent variables.\n",
    "\n",
    "2)Linearly combine the independent variables, such as adding them together.\n",
    "\n",
    "3)Partial least squares regression uses principal component analysis to create a set of uncorrelated components to include in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701a41f",
   "metadata": {},
   "source": [
    "# 20. What is polynomial regression and when is it used?\n",
    "\n",
    "A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression. It is used when linear regression models may not adequately capture the complexity of the relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949435d8",
   "metadata": {},
   "source": [
    "# Loss function:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586ffb5",
   "metadata": {},
   "source": [
    "# 21. What is a loss function and what is its purpose in machine learning?\n",
    "\n",
    " The Loss function is a method of evaluating how well your algorithm is modeling your dataset. \n",
    " \n",
    "  Used when we refer to the error for a single training example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab053f",
   "metadata": {},
   "source": [
    "# 22. What is the difference between a convex and non-convex loss function?\n",
    "\n",
    "A convex function: given any two points on the curve there will be no intersection with any other points, for non convex function there will be at least one intersection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ae0cf5",
   "metadata": {},
   "source": [
    "# 23. What is mean squared error (MSE) and how is it calculated?\n",
    "\n",
    "The Mean Squared Error measures how close a regression line is to a set of data points. It is a risk function corresponding to the expected value of the squared error loss. Mean square error is calculated by taking the average, specifically the mean, of errors squared from data as it relates to a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69735274",
   "metadata": {},
   "source": [
    "# 24. What is mean absolute error (MAE) and how is it calculated?\n",
    "\n",
    "\n",
    "MAE is calculated as the sum of absolute errors divided by the sample size: It is thus an arithmetic average of the absolute errors , where is the prediction and. the true value. Alternative formulations may include relative frequencies as weight factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a630d",
   "metadata": {},
   "source": [
    "# 25. What is log loss (cross-entropy loss) and how is it calculated?\n",
    "\n",
    "Also called logarithmic loss, log loss or logistic loss. Each predicted class probability is compared to the actual class desired output 0 or 1 and a score/loss is calculated that penalizes the probability based on how far it is from the actual expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5aa49e",
   "metadata": {},
   "source": [
    "# 26. How do you choose the appropriate loss function for a given problem?\n",
    "\n",
    "1)Regression Loss Functions\n",
    "\n",
    "Mean Squared Error Loss\n",
    "\n",
    "Mean Squared Logarithmic Error Loss\n",
    "\n",
    "Mean Absolute Error Loss\n",
    "\n",
    "2)Binary Classification Loss Functions\n",
    "\n",
    "Binary Cross-Entropy\n",
    "\n",
    "Hinge Loss\n",
    "\n",
    "Squared Hinge Loss\n",
    "\n",
    "3)Multi-Class Classification Loss Functions\n",
    "\n",
    "Multi-Class Cross-Entropy Loss\n",
    "\n",
    "Sparse Multiclass Cross-Entropy Loss\n",
    "\n",
    "Kullback Leibler Divergence Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bff514",
   "metadata": {},
   "source": [
    "# 27. Explain the concept of regularization in the context of loss functions.\n",
    "\n",
    "Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting. Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe4253",
   "metadata": {},
   "source": [
    "# 28. What is Huber loss and how does it handle outliers?\n",
    "\n",
    "the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74470ded",
   "metadata": {},
   "source": [
    "# 29. What is quantile loss and when is it used?\n",
    "\n",
    "a flexible loss function that can be incorporated into any regression model to predict a certain variable quantile. Based on the example of LightGBM, we saw how to adjust a model, so it solves a quantile regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe799d7e",
   "metadata": {},
   "source": [
    "# 30. What is the difference between squared loss and absolute loss?\n",
    "\n",
    "The MAE is a linear score which means that all the individual differences are weighted equally in the average. The RMSE is a quadratic scoring rule which measures the average magnitude of the error. The equation for the RMSE is given in both of the references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5f941",
   "metadata": {},
   "source": [
    "# Optimizer (GD):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa58b0",
   "metadata": {},
   "source": [
    "# 31. What is an optimizer and what is its purpose in machine learning?\n",
    "\n",
    "an act, process, or methodology of making something (such as a design, system, or decision) as fully perfect, functional, or effective as possible.\n",
    "\n",
    "An optimizer is an algorithm or function that adapts the neural network's attributes, like learning rate and weights. Hence, it assists in improving the accuracy and reduces the total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531e0ab",
   "metadata": {},
   "source": [
    "# 32. What is Gradient Descent (GD) and how does it work?\n",
    "\n",
    "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074d708",
   "metadata": {},
   "source": [
    "# 33. What are the different variations of Gradient Descent?\n",
    "\n",
    "Three simple variants of gradient descent algorithms, namely batch gradient descent, stochastic gradient descent and mini-batch gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add9c1e",
   "metadata": {},
   "source": [
    "# 34. What is the learning rate in GD and how do you choose an appropriate value?\n",
    "\n",
    "Learning rate (also referred to as step size or the alpha) is the size of the steps that are taken to reach the minimum. This is typically a small value, and it is evaluated and updated based on the behavior of the cost function\n",
    "\n",
    "smaller batch sizes are better suited to smaller learning rates given the noisy estimate of the error gradient. A traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf8178",
   "metadata": {},
   "source": [
    "# 35. How does GD handle local optima in optimization problems?\n",
    "\n",
    "Does gradient descent always converge to an optimum?\n",
    "Intuitively, this means that gradient descent is guaranteed to converge and that it converges with rate O(1/k). value strictly decreases with each iteration of gradient descent until it reaches the optimal value f(x) = f(x∗)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9257261",
   "metadata": {},
   "source": [
    "# 36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?\n",
    "In Gradient Descent (GD), we perform the forward pass using ALL the train data before starting the backpropagation pass to adjust the weights. This is called (one epoch). In Stochastic Gradient Descent (SGD), we perform the forward pass using a SUBSET of the train set followed by backpropagation to adjust the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61b55a",
   "metadata": {},
   "source": [
    "# 37. Explain the concept of batch size in GD and its impact on training.\n",
    "\n",
    "The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model's internal parameters are updated. The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb79628",
   "metadata": {},
   "source": [
    "# 38. What is the role of momentum in optimization algorithms?\n",
    "\n",
    "Momentum is an extension to the gradient descent optimization algorithm that allows the search to build inertia in a direction in the search space and overcome the oscillations of noisy gradients and coast across flat spots of the search space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08025ee",
   "metadata": {},
   "source": [
    "# 39. What is the difference between batch GD, mini-batch GD, and SGD?\n",
    "\n",
    "In the case of Stochastic Gradient Descent, we update the parameters after every single observation and we know that every time the weights are updated it is known as an iteration. In the case of Mini-batch Gradient Descent, we take a subset of data and update the parameters based on every subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9931cdc",
   "metadata": {},
   "source": [
    "# 40. How does the learning rate affect the convergence of GD?\n",
    "\n",
    "The learning rate determines how big the step would be on each iteration. If α is very small, it would take long time to converge and become computationally expensive. If α is large, it may fail to converge and overshoot the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07a5c8",
   "metadata": {},
   "source": [
    "# Regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365655f1",
   "metadata": {},
   "source": [
    "# 41. What is regularization and why is it used in machine learning?\n",
    "\n",
    "It is a technique to prevent the model from overfitting by adding extra information to it. Sometimes the machine learning model performs well with the training data but does not perform well with the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369651c4",
   "metadata": {},
   "source": [
    "# 42. What is the difference between L1 and L2 regularization?\n",
    "\n",
    "L1 regularization penalizes the sum of absolute values of the weights, whereas L2 regularization penalizes the sum of squares of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203c52d",
   "metadata": {},
   "source": [
    "# 43. Explain the concept of ridge regression and its role in regularization.\n",
    "\n",
    "Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50557d2",
   "metadata": {},
   "source": [
    "# 44. What is the elastic net regularization and how does it combine L1 and L2 penalties?\n",
    "\n",
    "The elastic net is a linear regression regularization technique that combines both the L1 (Lasso) and L2 (Ridge) regularization penalties. It is particularly useful when dealing with datasets that have high collinearity or when there are more predictors than observations. Where, m is number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56a91b",
   "metadata": {},
   "source": [
    "# 45. How does regularization help prevent overfitting in machine learning models?\n",
    "\n",
    "Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c43e33b",
   "metadata": {},
   "source": [
    "# 46. What is early stopping and how does it relate to regularization?\n",
    "\n",
    "In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4262219",
   "metadata": {},
   "source": [
    "# 47. Explain the concept of dropout regularization in neural networks\n",
    "\n",
    "Dropout is a regularization method approximating concurrent training of many neural networks with various designs. During training, some layer outputs are ignored or dropped at random. This makes the layer appear and is regarded as having a different number of nodes and connectedness to the preceding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92ca4b",
   "metadata": {},
   "source": [
    "# 48. How do you choose the regularization parameter in a model?\n",
    "\n",
    "on the training set, we estimate several different Ridge regressions, with different values of the regularization parameter; on the validation set, we choose the best model (the regularization parameter which gives the lowest MSE on the validation set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b08d48",
   "metadata": {},
   "source": [
    "# 49. What is the difference between feature selection and regularization?\n",
    "\n",
    "eature selection, also known as feature subset selection, variable selection, or attribute selection. This approach removes the dimensions (e.g. columns) from the input data and results in a reduced data set for model inference. Regularization, where we are constraining the solution space while doing optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf2ed7",
   "metadata": {},
   "source": [
    "# 50. What is the trade-off between bias and variance in regularized models?\n",
    "\n",
    "“Bias and variance are complements of each other” The increase of one will result in the decrease of the other and vice versa. Hence, finding the right balance of values is known as the Bias-Variance Tradeoff. An ideal algorithm should neither underfit nor overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd4c9f",
   "metadata": {},
   "source": [
    "# SVM:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9879b4df",
   "metadata": {},
   "source": [
    "# 51. What is Support Vector Machines (SVM) and how does it work?\n",
    "\n",
    "A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text.\n",
    "\n",
    "SVM works relatively well when there is a clear margin of separation between classes. SVM is more effective in high dimensional spaces and is relatively memory efficient. SVM is effective in cases where the dimensions are greater than the number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a4a0a",
   "metadata": {},
   "source": [
    "# 52. How does the kernel trick work in SVM?\n",
    "\n",
    "Kernel trick allows the inner product of mapping function instead of the data points. The trick is to identify the kernel functions which can be represented in place of the inner product of mapping functions. Kernel functions allow easy computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2164af02",
   "metadata": {},
   "source": [
    "# 53. What are support vectors in SVM and why are they important?\n",
    "\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae001a2",
   "metadata": {},
   "source": [
    "# 54. Explain the concept of the margin in SVM and its impact on model performance\n",
    "\n",
    "Margin: it is the distance between the hyperplane and the observations closest to the hyperplane (support vectors). In SVM large margin is considered a good margin. There are two types of margins hard margin and soft margin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e6424b",
   "metadata": {},
   "source": [
    "# 55. How do you handle unbalanced datasets in SVM?\n",
    "\n",
    "More specifically, an SVM classifier trained on an imbalanced dataset often produces models which are biased towards the majority class and have low performance on the mi- nority class. There have been various data preprocessing and algorithmic techniques proposed to overcome this problem for SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94bab03",
   "metadata": {},
   "source": [
    "# 56. What is the difference between linear SVM and non-linear SVM?\n",
    "\n",
    "Linear SVM: When the data points are linearly separable into two classes, the data is called linearly-separable data. We use the linear SVM classifier to classify such data. Non-linear SVM: When the data is not linearly separable, we use the non-linear SVM classifier to separate the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582f219",
   "metadata": {},
   "source": [
    "# 57. What is the role of C-parameter in SVM and how does it affect the decision boundary?\n",
    "\n",
    "C parameter adds a penalty for each misclassified data point. If c is small, the penalty for misclassified points is low so a decision boundary with a large margin is chosen at the expense of a greater number of misclassifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783b5d6",
   "metadata": {},
   "source": [
    "# 58. Explain the concept of slack variables in SVM.\n",
    "\n",
    "Slack variables are introduced to allow certain constraints to be violated. That is, certain train- ing points will be allowed to be within the margin. We want the number of points within the margin to be as small as possible, and of course we want their penetration of the margin to be as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d09aa",
   "metadata": {},
   "source": [
    "# 59. What is the difference between hard margin and soft margin in SVM?\n",
    "\n",
    "When the data is linearly separable, and we don't want to have any misclassifications, we use SVM with a hard margin. However, when a linear boundary is not feasible, or we want to allow some misclassifications in the hope of achieving better generality, we can opt for a soft margin for our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5740885",
   "metadata": {},
   "source": [
    "# 60. How do you interpret the coefficients in an SVM model?\n",
    "\n",
    " Recall that in linear SVM, the result is a hyperplane that separates the classes as best as possible. The weights represent this hyperplane, by giving you the coordinates of a vector which is orthogonal to the hyperplane - these are the coefficients given by svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed52e1",
   "metadata": {},
   "source": [
    "# Decision Trees:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4b50b1",
   "metadata": {},
   "source": [
    "# 61. What is a decision tree and how does it work?\n",
    "\n",
    "A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266364b",
   "metadata": {},
   "source": [
    "# 62. How do you make splits in a decision tree?\n",
    "\n",
    "Steps to split a decision tree using Information Gain:\n",
    "\n",
    "For each split, individually calculate the entropy of each child node. Calculate the entropy of each split as the weighted average entropy of child nodes. Select the split with the lowest entropy or highest information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d15880",
   "metadata": {},
   "source": [
    "# 63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?\n",
    "\n",
    "The Gini impurity measure is one of the methods used in decision tree algorithms to decide the optimal split from a root node, and subsequent splits. To put it into context, a decision tree is trying to create sequential questions such that it partitions the data into smaller groups.\n",
    "\n",
    "The Entropy and Information Gain method focuses on purity and impurity in a node. The Gini Index or Impurity measures the probability for a random instance being misclassified when chosen randomly. The lower the Gini Index, the better the lower the likelihood of misclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd30d20",
   "metadata": {},
   "source": [
    "# 64. Explain the concept of information gain in decision trees.\n",
    "\n",
    "Information gain is the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baee0bc",
   "metadata": {},
   "source": [
    "# 65. How do you handle missing values in decision trees?\n",
    "\n",
    "Surrogate splitting rules enable you to use the values of other input variables to perform a split for observations with missing values. Important Note : Tree Surrogate splitting rule method can impute missing values for both numeric and categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e16b68",
   "metadata": {},
   "source": [
    "# 66. What is pruning in decision trees and why is it important?\n",
    "\n",
    "Pruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd577799",
   "metadata": {},
   "source": [
    "# 67. What is the difference between a classification tree and a regression tree?\n",
    "The major difference between a classification tree and a regression tree is the nature of the variable to be predicted. In a regression tree, the variable is continuous rather than categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c53a57",
   "metadata": {},
   "source": [
    "# 68. How do you interpret the decision boundaries in a decision tree?\n",
    "\n",
    "The first node of the tree called the “root node” contains the number of instances of all the classes respectively. Basically, we have to draw a line called “decision boundary” that separates the instances of different classes into different regions called “decision regions”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0f175",
   "metadata": {},
   "source": [
    "# 69. What is the role of feature importance in decision trees?\n",
    "\n",
    "A decision tree is explainable machine learning algorithm all by itself. Beyond its transparency, feature importance is a common way to explain built models as well. Coefficients of linear regression equation give a opinion about feature importance but that would fail for non-linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbcce1e",
   "metadata": {},
   "source": [
    "# 70. What are ensemble techniques and how are they related to decision trees?\n",
    "\n",
    "Using one decision tree is can be problematic and might not be stable enough; however, using multiple decision trees and combining their results will do great. Combining multiple classifiers in a prediction model is called ensembling. The simple rule of ensemble methods is to reduce the error by reducing the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5140a35b",
   "metadata": {},
   "source": [
    "# Ensemble Techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad7435b",
   "metadata": {},
   "source": [
    "# 71. What are ensemble techniques in machine learning?\n",
    "\n",
    "An ensemble method is a technique which uses multiple independent similar or different models/weak learners to derive an output or make some predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c23a08",
   "metadata": {},
   "source": [
    "# 72. What is bagging and how is it used in ensemble learning?\n",
    "\n",
    "Bagging, also known as Bootstrap aggregating, is an ensemble learning technique that helps to improve the performance and accuracy of machine learning algorithms. It is used to deal with bias-variance trade-offs and reduces the variance of a prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d8c4c",
   "metadata": {},
   "source": [
    "# 73. Explain the concept of bootstrapping in bagging\n",
    "\n",
    "Bootstrap means that instead of training on all the observations, each tree of RF is trained on a subset of the observations. The chosen subset is called the bag, and the remaining are called Out of Bag samples. Multiple trees are trained on different bags, and later the results from all the trees are aggregated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73853b69",
   "metadata": {},
   "source": [
    "# 74. What is boosting and how does it work?\n",
    "\n",
    "Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e487bff",
   "metadata": {},
   "source": [
    "# 75. What is the difference between AdaBoost and Gradient Boosting?\n",
    "\n",
    "Adaboost is computed with a specific loss function and becomes more rigid when comes to few iterations. But in gradient boosting, it assists in finding the proper solution to additional iteration modeling problem as it is built with some generic features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0550d7",
   "metadata": {},
   "source": [
    "# 76. What is the purpose of random forests in ensemble learning?\n",
    "\n",
    "Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57986d3",
   "metadata": {},
   "source": [
    "# 77. How do random forests handle feature importance?\n",
    "\n",
    "The final feature importance, at the Random Forest level, is it's average over all the trees. The sum of the feature's importance value on each trees is calculated and divided by the total number of trees: RFfi sub(i)= the importance of feature i calculated from all trees in the Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967d33d",
   "metadata": {},
   "source": [
    "# 78. What is stacking in ensemble learning and how does it work?\n",
    "\n",
    "Stacking is one of the most popular ensemble machine learning techniques used to predict multiple nodes to build a new model and improve model performance\n",
    "\n",
    "Stacking involves training multiple base-models to predict the target variable in a machine learning problem while at the same time, a meta-model learns to use the predictions of each base model to predict the value of the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17911640",
   "metadata": {},
   "source": [
    "# 79. What are the advantages and disadvantages of ensemble techniques?\n",
    "\n",
    "advantages - \n",
    "Ensemble methods offer several advantages over single models, such as improved accuracy and performance, especially for complex and noisy problems. They can also reduce the risk of overfitting and underfitting by balancing the trade-off between bias and variance, and by using different subsets and features of the data.\n",
    "\n",
    "disadvantages - \n",
    "Ensembling is less interpretable, the output of the ensembled model is hard to predict and explain. ...\n",
    "The art of ensembling is hard to learn and any wrong selection can lead to lower predictive accuracy than an individual model.\n",
    "Ensembling is expensive in terms of both time and space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf5b18",
   "metadata": {},
   "source": [
    "# 80. How do you choose the optimal number of models in an ensemble?\n",
    "\n",
    "By hyperparameter tuning we can select optimal number of models for ensemble technic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bfd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
